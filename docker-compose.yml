version: "3"

services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - 9870:9870
      - 9000:9000
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env

  # secondary-namenode:
  #     image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
  #     container_name: secondary-namenode
  #     restart: always
  #     ports:
  #       - 9871:9870
  #       - 9001:9000
  #     volumes:
  #       - hadoop_namenode:/hadoop/dfs/name
  #     environment:
  #       - CLUSTER_NAME=test
  #     env_file:
  #       - ./hadoop.env
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    ports:
      - 9864:9864
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env
  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode1
    restart: always
    ports:
      - 9865:9864
    volumes:
      - hadoop_datanode1:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env
  
  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode2
    restart: always
    ports:
      - 9866:9864
    volumes:
      - hadoop_datanode2:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    restart: always
    ports:
      - 8088:8088
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 datanode1:9864 datanode2:9864"
    
    env_file:
      - ./hadoop.env

  nodemanager1:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 datanode1:9864 datanode2:9864 resourcemanager:8088"
    
    env_file:
      - ./hadoop.env
  
  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
    container_name: historyserver
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 datanode1:9864 datanode2:9864 resourcemanager:8088"
     
    volumes:
      - hadoop_historyserver:/hadoop/yarn/timeline
    env_file:
      - ./hadoop.env

  spark:
    image: bitnami/spark:latest
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - SPARK_USER=root
      - PYTHONPATH= '/opt/bitnami/spark/python/:/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip:/opt/bitnami/spark/python/lib/pyspark.zip:/opt/bitnami/spark/jars/elasticsearch-spark-30_2.13-8.11.3.jar:/opt/bitnami/spark/python/:'

    command: bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - '8080:8080'
      - '7077:7077'
    container_name: spark
    volumes:
      - spark:/spark


  
  spark-worker-1:
    image: bitnami/spark:latest
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    ports:
      - '8081:8081'
    depends_on:
      - spark
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark:7077
  spark-worker-2:
    image: bitnami/spark:latest
    container_name: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark:7077
    ports:
      - '8082:8081'
    depends_on:
      - spark
  spark-worker-3:
    image: bitnami/spark:latest
    container_name: spark-worker-3
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark:7077
    ports:
      - '8083:8081'
    depends_on:
      - spark

  elasticsearch:
    container_name: elasticsearch 
    image: docker.elastic.co/elasticsearch/elasticsearch:7.10.0
    environment:
      - discovery.type=single-node
      - node.name=elasticsearch
    ports:
      - 9200:9200
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data

  kibana:
    container_name: kibana
    image: docker.elastic.co/kibana/kibana:7.10.0
    ports:
      - 5601:5601
    depends_on:
      - elasticsearch


  # zookeeper:
  #   image: confluentinc/cp-zookeeper:latest
  #   container_name: zookeeper
  #   environment:
  #     ZOOKEEPER_CLIENT_PORT: 2181
  #     ZOOKEEPER_TICK_TIME: 2000
  #   ports:
  #     - 22181:2181

  # zookeeper-2:
  #   image: confluentinc/cp-zookeeper:latest
  #   container_name: zookeeper-2
  #   environment:
  #     ZOOKEEPER_CLIENT_PORT: 2181
  #     ZOOKEEPER_TICK_TIME: 2000
  #   ports:
  #     - 32181:2181
  
    # reachable on 9092 from the host and on 29092 from inside docker compose
  # kafka:
  #   image: confluentinc/cp-kafka:6.1.1
  #   container_name: kafka
  #   depends_on:
  #     - zookeeper
  #   ports:
  #     - '9092:9092'
  #   expose:
  #     - '29092'
  #   environment:
  #     KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
  #     KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
  #     KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
  #     KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
  #     KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: '1'
  #     KAFKA_MIN_INSYNC_REPLICAS: '1'

volumes:
  hadoop_namenode:
  hadoop_datanode:
  hadoop_datanode1:
  hadoop_datanode2:
  hadoop_historyserver:
  spark:
  spark-lab:
  elasticsearch-data: 